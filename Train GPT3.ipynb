{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19172,"status":"ok","timestamp":1676395803821,"user":{"displayName":"Wang Meng","userId":"10347770255541938362"},"user_tz":300},"id":"wFcdMa4ryqpE","outputId":"5185e66a-e726-49da-d617-65f7bed57f0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#import data from google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1udfx9SzOdO"},"outputs":[],"source":["import pandas as pd\n","import string\n","import json\n","import warnings\n","warnings.filterwarnings(action = 'ignore')\n","import itertools\n","import numpy as np\n","import math\n","from numpy import dot\n","from numpy.linalg import norm\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","!pip install datasets\n","from datasets import Dataset\n","\n","!pip install --upgrade openai\n","import openai"]},{"cell_type":"markdown","metadata":{"id":"eLMQZOvn6dFL"},"source":["**Train ML Model--Contributor**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOEfzt8EM9wR"},"outputs":[],"source":["##### read labeled sentences\n","df_train = pd.read_excel(\"\\sample_data.xlsx\")\n","\n","for var in ['pa','con_det',\t'sp_non']:\n","    df_train [var] = df_train [var].fillna(-999)\n","    df_train [var] = df_train [var].astype(int)\n","\n","df_train_pa = df_train[df_train['pa'] != -999]\n","df_train_pa = df_train_pa[['sentence','pa']]\n","\n","df_train_con_det = df_train[df_train['con_det'] != -999]\n","df_train_con_det = df_train_con_det[['sentence','con_det']]\n","df_train_con_det.reset_index(drop=True, inplace=True)\n","\n","df_train_sp_non = df_train[df_train['sp_non'] != -999]\n","df_train_sp_non = df_train_sp_non[['sentence','sp_non']]\n","df_train_sp_non.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hapF_-FM6dFM"},"outputs":[],"source":["##### read labeled sentences\n","df_train = pd.read_excel(\".xlsx\")\n","\n","for var in ['pa','con_det',\t'sp_non']:\n","    df_train [var] = df_train [var].fillna(-999)\n","    df_train [var] = df_train [var].astype(int)\n","\n","df_train_pa = df_train[df_train['pa'] != -999]\n","df_train_pa = df_train_pa[['sentence','pa']]\n","\n","df_train_con_det = df_train[df_train['con_det'] != -999]\n","df_train_con_det = df_train_con_det[['sentence','con_det']]\n","df_train_con_det.reset_index(drop=True, inplace=True)\n","\n","df_train_sp_non = df_train[df_train['sp_non'] != -999]\n","df_train_sp_non = df_train_sp_non[df_train_sp_non['con_det'] == 1] ################################ conditional ############################\n","df_train_sp_non = df_train_sp_non[['sentence','sp_non']]\n","df_train_sp_non.reset_index(drop=True, inplace=True)\n","\n","df_train = pd.read_excel(\".xlsx\")\n","\n","for var in ['pa','con_det',\t'sp_non']:\n","    df_train [var] = df_train [var].fillna(-999)\n","    df_train [var] = df_train [var].astype(int)\n","\n","df_train['pa'] = 0\n","df_train.loc[df_train.con_det > -999, 'pa'] = 1\n","df_train_pa2 = df_train[df_train['pa'] != -999]\n","df_train_pa2 = df_train_pa2[['sentence','pa']]\n","\n","df_train_con_det2 = df_train[df_train['con_det'] != -999]\n","df_train_con_det2 = df_train_con_det2[['sentence','con_det']]\n","df_train_con_det2.reset_index(drop=True, inplace=True)\n","\n","df_train_sp_non2 = df_train[df_train['sp_non'] != -999]\n","df_train_sp_non2 = df_train_sp_non2[df_train_sp_non2['con_det'] == 1] ################################ conditional ############################\n","df_train_sp_non2 = df_train_sp_non2[['sentence','sp_non']]\n","df_train_sp_non2.reset_index(drop=True, inplace=True)\n","\n","df_train_pa = df_train_pa.append(df_train_pa2)\n","df_train_con_det = df_train_con_det.append(df_train_con_det2)\n","df_train_sp_non = df_train_sp_non.append(df_train_sp_non2)\n","\n","\n","df_train_pa.reset_index(drop=True, inplace=True)\n","df_train_con_det.reset_index(drop=True, inplace=True)\n","df_train_sp_non.reset_index(drop=True, inplace=True)\n","\n","'''###################### contributor labels ######################'''\n","# form a training set--contributor/detractor\n","df_train2 = Dataset.from_pandas(df_train_con_det)\n","df_train2 = df_train2.rename_column('con_det','label')\n","df_train2\n","\n","'''###################### bert ######################'''\n","Tokenizer_name = \"bert-base-uncased\"                       ################################ model selection ############################\n","Model_name = \"bert-base-uncased\"                           ################################ model selection ############################\n","Number_of_labels = 2\n","\n","!pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(Tokenizer_name)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = df_train2.map(tokenize_function, batched=True)\n","\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","\n","\n","\n","#Stratified k-fold sampling and modeling\n","label_for_s_kfold = tokenized_datasets['label']\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","from transformers import TrainingArguments\n","\n","from datasets import load_metric\n","training_args = TrainingArguments(\"test_trainer\")\n","\n","import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","from transformers import Trainer\n","\n","\n","s_kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False) #set k =5\n","my_accuracy = []\n","my_precison = []\n","my_recall = []\n","my_fscore = []\n","my_confusion_matrix = []\n","\n","for train_index, test_index in s_kfold.split(np.zeros(len(label_for_s_kfold)), label_for_s_kfold):\n","  my_train_set = tokenized_datasets.select(train_index)\n","  my_test_set = tokenized_datasets.select(test_index )\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(Model_name, num_labels = Number_of_labels)\n","\n","  trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=my_train_set,\n","    eval_dataset=my_test_set,\n","    compute_metrics=compute_metrics,\n",")\n","  trainer.train()\n","\n","  predictions = trainer.predict(my_test_set)\n","  array_predict = predictions.predictions\n","  list_actual = my_test_set['label']\n","  list_predict = []\n","  for i in range(array_predict.shape[0]):\n","    list_predict.append(np.argmax(array_predict[i]))\n","\n","  my_precison_new, my_recall_new, my_fscore_new, my_support_new = precision_recall_fscore_support(list_actual, list_predict, average='weighted')\n","\n","  my_accuracy.append(trainer.evaluate()['eval_accuracy'])\n","\n","  my_precison.append(my_precison_new)\n","  my_recall.append(my_recall_new)\n","  my_fscore.append(my_fscore_new)\n","\n","  my_confusion_matrix.append(confusion_matrix(list_actual,list_predict))\n","\n","print(\"fscore:\", sum(my_fscore)/len(my_fscore))\n","print(\"confusion_matrix:\\n\", sum(my_confusion_matrix) / len(my_confusion_matrix))\n","\n","\n","\n","\n","\n","\n","'''###################### get the relevant sentence ######################'''\n","'''load sentence count'''\n","dfn = pd.read_csv(\".csv\")\n","dfn.reset_index(drop=True, inplace=True)\n","\n","'''contrsuct sentence corpus'''\n","dfn_original = pd.read_csv(\".csv\")\n","\n","repeat_times = []\n","mycorpus_sent = []\n","\n","for index,row in dfn_original.iterrows():\n","    repeat_times.append(len(sent_tokenize(dfn_original['discussion'][index])))\n","    for my_sent in sent_tokenize(dfn_original['discussion'][index]):\n","\n","        mycorpus_sent.append(my_sent)\n","\n","dfn_sentence = dfn_original[['cik','date_filing']]\n","dfn_sentence = dfn_sentence.loc[dfn_original.index.repeat(repeat_times)]\n","\n","dfn_sentence['sentence'] = mycorpus_sent\n","dfn_sentence.reset_index(drop=True, inplace=True)\n","\n","\n","\n","'''###################### sentence prediction ######################'''\n","dfn_perfprmance_index = dfn.index[dfn.drop(['cik','date_filing','total_word'], axis=1).sum(axis = 1) > 0]\n","dfn_performance = dfn_sentence[dfn_sentence.index.isin(dfn_perfprmance_index)]\n","\n","dfn_performance_2 = dfn_performance[['cik','date_filing']]\n","df_test1 = dfn_performance[['sentence']]\n","df_test2 = Dataset.from_pandas(df_test1)\n","\n","\n","tokenized_datasets = df_test2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","my_test_set = tokenized_datasets\n","\n","# predict using NLP model\n","predictions = trainer.predict(my_test_set)\n","array_predict = predictions.predictions\n","\n","list_predict = []\n","for i in range(array_predict.shape[0]):\n","  list_predict.append(np.argmax(array_predict[i]))\n","\n","dfn_performance_2[\"con_det\"] = list_predict\n","\n","\n","#save the results\n","dfn_performance_2.reset_index(inplace=True)\n","dfn_performance_2 = dfn_performance_2.rename(columns = {'index':'orginal_index'})\n","dfn_performance_2.to_csv(\".csv\",index = False)\n"]},{"cell_type":"markdown","metadata":{"id":"gRbiZHZDKcZ0"},"source":["**GPT3**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jforc5rjDG4G"},"outputs":[],"source":["'''###################### get the relevant sentence ######################'''\n","'''load sentence count'''\n","dfn = pd.read_csv(\".csv\")\n","dfn.reset_index(drop=True, inplace=True)\n","\n","'''contrsuct sentence corpus'''\n","dfn_original = pd.read_csv(\".csv\")\n","\n","repeat_times = []\n","mycorpus_sent = []\n","\n","for index,row in dfn_original.iterrows():\n","    repeat_times.append(len(sent_tokenize(dfn_original['discussion'][index])))\n","    for my_sent in sent_tokenize(dfn_original['discussion'][index]):\n","\n","        mycorpus_sent.append(my_sent)\n","\n","dfn_sentence = dfn_original[['cik','date_filing']]\n","dfn_sentence = dfn_sentence.loc[dfn_original.index.repeat(repeat_times)]\n","\n","dfn_sentence['sentence'] = mycorpus_sent\n","dfn_sentence.reset_index(drop=True, inplace=True)\n","\n","'''###################### contrcut predict set ######################'''\n","dfn_perfprmance_index = dfn.index[dfn.drop(['cik','date_filing','total_word'], axis=1).sum(axis = 1) > 0]\n","dfn_performance = dfn_sentence[dfn_sentence.index.isin(dfn_perfprmance_index)]\n","\n","dfn_performance_2 = dfn_performance[['cik','date_filing']]\n","df_test = dfn_performance[['sentence']]\n","df_test.reset_index(inplace=True)\n","\n","dfn_performance_2.reset_index(inplace=True)\n","dfn_performance_2 = dfn_performance_2.rename(columns = {'index':'orginal_index'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XNqysK1FBk5"},"outputs":[],"source":["'''###################### choose model ######################'''\n","openai.api_key =\"\"\n","\n","ft_model = ''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QYqcpUOG25Z"},"outputs":[],"source":["'''###################### sentence prediction ######################'''\n","div_obs = 5000\n","last_part_n_saw = 10\n","\n","for i in range(last_part_n_saw, df_test.shape[0]//div_obs + 1):\n","  # index & corpus subsample\n","  dfn_performance_2_sub = dfn_performance_2[i*div_obs:(i+1)*div_obs]\n","  dfn_performance_2_sub.reset_index(drop=True, inplace=True)\n","\n","  df_test_sub = df_test[i*div_obs:(i+1)*div_obs]\n","  df_test_sub.reset_index(drop=True, inplace=True)\n","\n","  list_predict = []\n","  for index,row in df_test_sub.iterrows():\n","    my_prompt = df_test_sub['sentence'][index] + \" ->\"\n","\n","    res = openai.Completion.create(model=ft_model, prompt=my_prompt, max_tokens=1, temperature=0)\n","\n","    list_predict.append(res['choices'][0]['text'])\n","\n","  dfn_performance_2_sub[\"con_det\"] = list_predict\n","  dfn_performance_2_sub.to_csv(\"\"+str(i + 1)+\".csv\",index = False)\n"]},{"cell_type":"code","source":["#append all parts\n","df_train_sub = pd.read_csv(\"\")\n","for i in range(1, dfn_performance_2.shape[0]//div_obs + 1):\n","  df_train_sub_next = pd.read_csv(\"\"+str(i + 1)+\".csv\")\n","  df_train_sub = df_train_sub.append(df_train_sub_next)\n","\n","df_train_sub.to_csv(\".csv\",index = False)"],"metadata":{"id":"qO2bT6ICCowH"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMLE5rTgc1Vs37CaA+CSELi"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}