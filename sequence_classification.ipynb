{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wFcdMa4ryqpE"},"outputs":[],"source":["# connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1udfx9SzOdO"},"outputs":[],"source":["import pandas as pd\n","import string\n","import json\n","import warnings\n","warnings.filterwarnings(action = 'ignore')\n","import itertools\n","import numpy as np\n","import math\n","from numpy import dot\n","from numpy.linalg import norm\n","\n","#import nltk\n","#nltk.download('punkt')\n","#from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","!pip install datasets\n","from datasets import Dataset"]},{"cell_type":"markdown","metadata":{"id":"eLMQZOvn6dFL"},"source":["**BERT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOEfzt8EM9wR"},"outputs":[],"source":["############################## Read Data ##############################\n","#######################################################################\n","# read data\n","df_train1 = pd.read_excel(\"/content/drive/MyDrive/sample_data.xlsx\")\n","df_train1.reset_index(drop=True, inplace=True)\n","\n","df_train2 = Dataset.from_pandas(df_train1)\n","df_train2 = df_train2.rename_column('contributor','label')\n","df_train2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hapF_-FM6dFM"},"outputs":[],"source":["############################## Train Model ##############################\n","#########################################################################\n","!pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# model selection\n","Tokenizer_name = \"bert-base-uncased\"\n","Model_name = \"bert-base-uncased\"\n","Number_of_labels = 2\n","\n","tokenizer = AutoTokenizer.from_pretrained(Tokenizer_name)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = df_train2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","\n","# set up stratified 5-fold sampling\n","label_for_s_kfold = tokenized_datasets['label']\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","from transformers import TrainingArguments\n","\n","from datasets import load_metric\n","training_args = TrainingArguments(\"test_trainer\")\n","\n","import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","from transformers import Trainer\n","\n","s_kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False) # choose n\n","my_accuracy = []\n","my_precison = []\n","my_recall = []\n","my_fscore = []\n","my_confusion_matrix = []\n","\n","for train_index, test_index in s_kfold.split(np.zeros(len(label_for_s_kfold)), label_for_s_kfold):\n","  my_train_set = tokenized_datasets.select(train_index)\n","  my_test_set = tokenized_datasets.select(test_index )\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(Model_name, num_labels = Number_of_labels)\n","\n","  trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=my_train_set,\n","    eval_dataset=my_test_set,\n","    compute_metrics=compute_metrics,\n",")\n","  trainer.train()\n","\n","  predictions = trainer.predict(my_test_set)\n","  array_predict = predictions.predictions\n","  list_actual = my_test_set['label']\n","  list_predict = []\n","  for i in range(array_predict.shape[0]):\n","    list_predict.append(np.argmax(array_predict[i]))\n","\n","  my_precison_new, my_recall_new, my_fscore_new, my_support_new = precision_recall_fscore_support(list_actual, list_predict, average='weighted')\n","\n","  my_accuracy.append(trainer.evaluate()['eval_accuracy'])\n","\n","  my_precison.append(my_precison_new)\n","  my_recall.append(my_recall_new)\n","  my_fscore.append(my_fscore_new)\n","\n","  my_confusion_matrix.append(confusion_matrix(list_actual,list_predict))\n","\n","print(\"fscore:\", sum(my_fscore)/len(my_fscore))\n","print(\"confusion_matrix:\\n\", sum(my_confusion_matrix) / len(my_confusion_matrix))\n"]},{"cell_type":"code","source":["############################## Predict #################################\n","########################################################################\n","df_test1 = pd.read_excel(\"/content/drive/MyDrive/your_data.xlsx\")\n","df_test1.reset_index(drop=True, inplace=True)\n","df_test1 = df_test1[['sentence']]\n","df_test2 = Dataset.from_pandas(df_test1)\n","\n","tokenized_datasets = df_test2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","my_test_set = tokenized_datasets\n","\n","# predict using trained model\n","predictions = trainer.predict(my_test_set)\n","array_predict = predictions.predictions\n","\n","list_predict = []\n","for i in range(array_predict.shape[0]):\n","  list_predict.append(np.argmax(array_predict[i]))\n","\n","df_test1[\"contributor\"] = list_predict\n","df_test1"],"metadata":{"id":"iSSSq365C-d-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RoBERTa**"],"metadata":{"id":"jYfvlFqLF6sV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jWFPYwmbF-eI"},"outputs":[],"source":["############################## Read Data ##############################\n","#######################################################################\n","# read data\n","df_train1 = pd.read_excel(\"/content/drive/MyDrive/sample_data.xlsx\")\n","df_train1.reset_index(drop=True, inplace=True)\n","\n","df_train2 = Dataset.from_pandas(df_train1)\n","df_train2 = df_train2.rename_column('contributor','label')\n","df_train2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20P_acRWF-eI"},"outputs":[],"source":["############################## Train Model ##############################\n","#########################################################################\n","!pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# model selection\n","Tokenizer_name = \"roberta-base\"\n","Model_name = \"roberta-base\"\n","Number_of_labels = 2\n","\n","tokenizer = AutoTokenizer.from_pretrained(Tokenizer_name)\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_datasets = df_train2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","\n","# set up stratified 5-fold sampling\n","label_for_s_kfold = tokenized_datasets['label']\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","from transformers import TrainingArguments\n","\n","from datasets import load_metric\n","training_args = TrainingArguments(\"test_trainer\")\n","\n","import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","from transformers import Trainer\n","\n","s_kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False) # choose n\n","my_accuracy = []\n","my_precison = []\n","my_recall = []\n","my_fscore = []\n","my_confusion_matrix = []\n","\n","for train_index, test_index in s_kfold.split(np.zeros(len(label_for_s_kfold)), label_for_s_kfold):\n","  my_train_set = tokenized_datasets.select(train_index)\n","  my_test_set = tokenized_datasets.select(test_index )\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(Model_name, num_labels = Number_of_labels)\n","\n","  trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=my_train_set,\n","    eval_dataset=my_test_set,\n","    compute_metrics=compute_metrics,\n",")\n","  trainer.train()\n","\n","  predictions = trainer.predict(my_test_set)\n","  array_predict = predictions.predictions\n","  list_actual = my_test_set['label']\n","  list_predict = []\n","  for i in range(array_predict.shape[0]):\n","    list_predict.append(np.argmax(array_predict[i]))\n","\n","  my_precison_new, my_recall_new, my_fscore_new, my_support_new = precision_recall_fscore_support(list_actual, list_predict, average='weighted')\n","\n","  my_accuracy.append(trainer.evaluate()['eval_accuracy'])\n","\n","  my_precison.append(my_precison_new)\n","  my_recall.append(my_recall_new)\n","  my_fscore.append(my_fscore_new)\n","\n","  my_confusion_matrix.append(confusion_matrix(list_actual,list_predict))\n","\n","print(\"fscore:\", sum(my_fscore)/len(my_fscore))\n","print(\"confusion_matrix:\\n\", sum(my_confusion_matrix) / len(my_confusion_matrix))\n"]},{"cell_type":"code","source":["############################## Predict #################################\n","########################################################################\n","df_test1 = pd.read_excel(\"/content/drive/MyDrive/your_data.xlsx\")\n","df_test1.reset_index(drop=True, inplace=True)\n","df_test1 = df_test1[['sentence']]\n","df_test2 = Dataset.from_pandas(df_test1)\n","\n","tokenized_datasets = df_test2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","my_test_set = tokenized_datasets\n","\n","# predict using trained model\n","predictions = trainer.predict(my_test_set)\n","array_predict = predictions.predictions\n","\n","list_predict = []\n","for i in range(array_predict.shape[0]):\n","  list_predict.append(np.argmax(array_predict[i]))\n","\n","df_test1[\"contributor\"] = list_predict\n","df_test1"],"metadata":{"id":"wqLs6JtgF-eI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**GPT-2**"],"metadata":{"id":"9OOSl5CxIP3U"}},{"cell_type":"code","source":["############################## Read Data ##############################\n","#######################################################################\n","# read data\n","df_train1 = pd.read_excel(\"/content/drive/MyDrive/sample_data.xlsx\")\n","df_train1.reset_index(drop=True, inplace=True)\n","\n","df_train2 = Dataset.from_pandas(df_train1)\n","df_train2 = df_train2.rename_column('contributor','label')\n","df_train2"],"metadata":{"id":"hcY9-y8HJaoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Train Model ##############################\n","#########################################################################\n","!pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","# model selection\n","Tokenizer_name = \"gpt2\"\n","Model_name = \"gpt2\"\n","Number_of_labels = 2\n","\n","#pip install transformers\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(Tokenizer_name)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"sentence\"],padding=\"max_length\", truncation=True, max_length = 57)\n","\n","tokenized_datasets = df_train2.map(tokenize_function, batched=True)\n","\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","\n","# set up stratified 5-fold sampling\n","label_for_s_kfold = tokenized_datasets['label']\n","from sklearn.model_selection import StratifiedKFold\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","from transformers import TrainingArguments\n","\n","from datasets import load_metric\n","training_args = TrainingArguments(\"test_trainer\")\n","\n","import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","from transformers import Trainer\n","\n","\n","s_kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False) #set k =5\n","my_accuracy = []\n","my_precison = []\n","my_recall = []\n","my_fscore = []\n","my_confusion_matrix = []\n","\n","for train_index, test_index in s_kfold.split(np.zeros(len(label_for_s_kfold)), label_for_s_kfold):\n","  my_train_set = tokenized_datasets.select(train_index)\n","  my_test_set = tokenized_datasets.select(test_index )\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(Model_name, num_labels = Number_of_labels)\n","  model.config.pad_token_id = model.config.eos_token_id\n","\n","  trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=my_train_set,\n","    eval_dataset=my_test_set,\n","    compute_metrics=compute_metrics,\n",")\n","  trainer.train()\n","\n","  predictions = trainer.predict(my_test_set)\n","  array_predict = predictions.predictions\n","  list_actual = my_test_set['label']\n","  list_predict = []\n","  for i in range(array_predict.shape[0]):\n","    list_predict.append(np.argmax(array_predict[i]))\n","\n","  my_precison_new, my_recall_new, my_fscore_new, my_support_new = precision_recall_fscore_support(list_actual, list_predict, average='weighted')\n","\n","  my_accuracy.append(trainer.evaluate()['eval_accuracy'])\n","\n","  my_precison.append(my_precison_new)\n","  my_recall.append(my_recall_new)\n","  my_fscore.append(my_fscore_new)\n","\n","  my_confusion_matrix.append(confusion_matrix(list_actual,list_predict))\n","\n","print(\"fscore:\", sum(my_fscore)/len(my_fscore))\n","print(\"confusion_matrix:\\n\", sum(my_confusion_matrix) / len(my_confusion_matrix))"],"metadata":{"id":"8RXOl069ISTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Predict #################################\n","########################################################################\n","df_test1 = pd.read_excel(\"/content/drive/MyDrive/your_data.xlsx\")\n","df_test1.reset_index(drop=True, inplace=True)\n","df_test1 = df_test1[['sentence']]\n","df_test2 = Dataset.from_pandas(df_test1)\n","\n","tokenized_datasets = df_test2.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['sentence'])\n","my_test_set = tokenized_datasets\n","\n","# predict using trained model\n","predictions = trainer.predict(my_test_set)\n","array_predict = predictions.predictions\n","\n","list_predict = []\n","for i in range(array_predict.shape[0]):\n","  list_predict.append(np.argmax(array_predict[i]))\n","\n","df_test1[\"contributor\"] = list_predict\n","df_test1"],"metadata":{"id":"EUQg7Un8JskR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**GPT-3**"],"metadata":{"id":"YHtOEKOjJyf5"}},{"cell_type":"code","source":["############################## Read Data ##############################\n","#######################################################################\n","# Enter credentials\n","import openai\n","%env OPENAI_API_KEY = your_key\n","\n","# read data\n","df_train1 = pd.read_excel(\"/content/drive/MyDrive/sample_data.xlsx\")\n","df_train1.reset_index(drop=True, inplace=True)\n","\n","df_train1.columns = ['prompt','completion']\n","df_train1.to_json(\"/content/drive/MyDrive/df_train_final.jsonl\", orient='records', lines=True)\n","!openai tools fine_tunes.prepare_data -f \"/content/drive/MyDrive/df_train_final.jsonl\""],"metadata":{"id":"KFJP-cDBKP9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Train Model ##############################\n","#########################################################################\n","!openai api fine_tunes.create \\\n","    -t \"/content/drive/MyDrive/df_train_final_prepared_train.jsonl\" \\\n","    -v \"/content/drive/MyDrive/df_train_final_prepared_valid.jsonl\" \\\n","    --compute_classification_metrics \\\n","    --classification_positive_class \\\n","    \" 0\" \\\n","    -m ada \\ # model selection\n","    #--n_epochs 4 \\\n","    #--batch_size $batch_size \\\n","    #--learning_rate_multiplier $learning_rate_multiplier \\\n","    #--prompt_loss_weight $prompt_loss_weight"],"metadata":{"id":"fPOFifsaKJ1z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# continue fine-tune\n","!openai api fine_tunes.follow -i your_model_id"],"metadata":{"id":"z2v5eTaAK6ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save results\n","!openai api fine_tunes.results -i ft-uIXlcKUcurKy9uMjUCsV5s7G > \"/content/drive/MyDrive/GPT3_ft/df_train_final_done.csv\"\n","results = pd.read_csv(\"/content/drive/MyDrive/GPT3_ft/df_train_final_done.csv\")\n","results[results['classification/accuracy'].notnull()].tail(1)"],"metadata":{"id":"nzVl7AuFLGjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["############################## Predict #################################\n","########################################################################\n","# set up your model\n","openai.api_key =\"your_key\"\n","ft_model = 'your_model'\n","\n","# divide large data into small parts and predict\n","df_test = pd.read_excel(\"/content/drive/MyDrive/your_data.xlsx\")\n","df_test.reset_index(drop=True, inplace=True)\n","df_test = df_test[['sentence']]\n","\n","\n","\n","div_obs = 5000\n","last_part_n_saw = 10 # set up starting point\n","\n","for i in range(last_part_n_saw, df_test.shape[0]//div_obs + 1):\n","  # index & corpus subsample\n","\n","  df_test_sub = df_test[i*div_obs:(i+1)*div_obs]\n","  df_test_sub.reset_index(drop=True, inplace=True)\n","\n","  list_predict = []\n","  for index,row in df_test_sub.iterrows():\n","    my_prompt = df_test_sub['sentence'][index] + \" ->\"\n","\n","    res = openai.Completion.create(model=ft_model, prompt=my_prompt, max_tokens=1, temperature=0)\n","\n","    list_predict.append(res['choices'][0]['text'])\n","\n","  df_test[\"contributor\"] = list_predict\n","  df_test.to_csv(\"/content/drive/MyDrive/your_data_part\"+str(i + 1)+\".csv\",index = False)\n","\n","\n","# append all parts\n","df_test_all = pd.read_csv(\"/content/drive/MyDrive/your_data_part1.csv\")\n","for i in range(1, df_test.shape[0]//div_obs + 1):\n","  df_test_all_next = pd.read_csv(\"/content/drive/MyDrive/your_data_part\"+str(i + 1)+\".csv\")\n","  df_test_all = df_test_all.append(df_test_all_next)\n","\n","df_test_all"],"metadata":{"id":"3a0hb_YrLbLN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOwyxwsnQedJOCwFc1/ujYJ"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}